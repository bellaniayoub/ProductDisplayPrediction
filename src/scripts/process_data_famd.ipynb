{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a83332c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import prince (FAMD implementation)\n",
    "try:\n",
    "    import prince\n",
    "    PRINCE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PRINCE_AVAILABLE = False\n",
    "    print(\"Warning: 'prince' package not found. Install it with: pip install prince\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad9c0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA PROCESSING WITH FAMD\n",
      "================================================================================\n",
      "\n",
      "1. Loading raw data...\n",
      "   ✓ Loaded 25782 rows and 8 columns\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. LOAD DATA\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA PROCESSING WITH FAMD\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. Loading raw data...\")\n",
    "data_path = '../../Data/raw/new_Base_CDM_balanced_V2.csv'\n",
    "\n",
    "# Load data (skip descriptive label row)\n",
    "df = pd.read_csv(data_path, sep=';', skiprows=[1])\n",
    "print(f\"   ✓ Loaded {len(df)} rows and {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7827b13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Preparing data...\n",
      "   ✓ Removed X1 and X2\n",
      "   ✓ Continuous variables: ['X3', 'X4', 'X6']\n",
      "   ✓ Categorical variables: ['X5', 'X7']\n",
      "   ✓ Data shape after removal: (25782, 6)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 2. PREPARE DATA - REMOVE X1, X2\n",
    "# ============================================================================\n",
    "print(\"\\n2. Preparing data...\")\n",
    "\n",
    "# Target variable\n",
    "target = 'Y'\n",
    "\n",
    "# Variables to keep\n",
    "# Continuous: X3, X4, X6 (X1, X2 removed)\n",
    "# Categorical: X5, X7\n",
    "continuous_vars = ['X3', 'X4', 'X6']\n",
    "categorical_vars = ['X5', 'X7']\n",
    "\n",
    "# Remove X1 and X2\n",
    "df_processed = df.drop(['X1', 'X2'], axis=1)\n",
    "\n",
    "# Convert continuous variables to numeric\n",
    "for col in continuous_vars:\n",
    "    df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
    "\n",
    "print(f\"   ✓ Removed X1 and X2\")\n",
    "print(f\"   ✓ Continuous variables: {continuous_vars}\")\n",
    "print(f\"   ✓ Categorical variables: {categorical_vars}\")\n",
    "print(f\"   ✓ Data shape after removal: {df_processed.shape}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing = df_processed[continuous_vars].isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(f\"\\n   ⚠ Warning: Missing values in continuous variables:\")\n",
    "    print(missing[missing > 0])\n",
    "    # Fill missing values with median\n",
    "    for col in continuous_vars:\n",
    "        if df_processed[col].isnull().sum() > 0:\n",
    "            median_val = df_processed[col].median()\n",
    "            df_processed[col].fillna(median_val, inplace=True)\n",
    "            print(f\"   ✓ Filled missing values in {col} with median: {median_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8d11ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Normalizing continuous variables...\n",
      "   ✓ Applied StandardScaler (mean=0, std=1)\n",
      "   ✓ Normalized variables: ['X3_norm', 'X4_norm', 'X6_norm']\n",
      "\n",
      "   Normalization statistics:\n",
      "     X3:\n",
      "       Original - Mean: 64641.18, Std: 54924.65\n",
      "       Normalized - Mean: -0.0000, Std: 1.0000\n",
      "     X4:\n",
      "       Original - Mean: 37.65, Std: 23.50\n",
      "       Normalized - Mean: 0.0000, Std: 1.0000\n",
      "     X6:\n",
      "       Original - Mean: 587.86, Std: 1821.34\n",
      "       Normalized - Mean: -0.0000, Std: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3. NORMALIZE CONTINUOUS VARIABLES\n",
    "# ============================================================================\n",
    "print(\"\\n3. Normalizing continuous variables...\")\n",
    "\n",
    "# Extract continuous variables\n",
    "X_continuous = df_processed[continuous_vars].copy()\n",
    "\n",
    "# Apply StandardScaler (Z-score normalization: mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_continuous_scaled = scaler.fit_transform(X_continuous)\n",
    "X_continuous_scaled_df = pd.DataFrame(\n",
    "    X_continuous_scaled,\n",
    "    columns=[f'{col}_norm' for col in continuous_vars],\n",
    "    index=df_processed.index\n",
    ")\n",
    "\n",
    "print(\"   ✓ Applied StandardScaler (mean=0, std=1)\")\n",
    "print(f\"   ✓ Normalized variables: {list(X_continuous_scaled_df.columns)}\")\n",
    "print(f\"\\n   Normalization statistics:\")\n",
    "for i, col in enumerate(continuous_vars):\n",
    "    print(f\"     {col}:\")\n",
    "    print(f\"       Original - Mean: {X_continuous[col].mean():.2f}, Std: {X_continuous[col].std():.2f}\")\n",
    "    print(f\"       Normalized - Mean: {X_continuous_scaled_df[f'{col}_norm'].mean():.4f}, Std: {X_continuous_scaled_df[f'{col}_norm'].std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fdf6d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. One-hot encoding categorical variables...\n",
      "   ✓ Applied One-Hot Encoding (drop='first')\n",
      "   ✓ Encoded variables: ['X5_CARREFOUR', 'X5_CARREFOUR MARKET', 'X5_CASINO', 'X5_CORA', 'X5_ECOMARCHE', 'X5_FRANPRIX', 'X5_GEANT', 'X5_HYPER U', 'X5_INTERMARCHE', 'X5_LECLERC', 'X5_MARCHE U', 'X5_MATCH', 'X5_MONOPRIX', 'X5_OTHERS', 'X5_PRISUNIC', 'X5_SHOPI', 'X5_SIMPLY MARKET', 'X5_SUPER U', 'X7_No_Feat']\n",
      "   ✓ Number of encoded features: 19\n",
      "\n",
      "   Encoding details:\n",
      "     X5: 19 categories → 18 encoded features\n",
      "     X7: 2 categories → 1 encoded features\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 4. ONE-HOT ENCODE CATEGORICAL VARIABLES\n",
    "# ============================================================================\n",
    "print(\"\\n4. One-hot encoding categorical variables...\")\n",
    "\n",
    "# Extract categorical variables\n",
    "X_categorical = df_processed[categorical_vars].copy()\n",
    "\n",
    "# Apply one-hot encoding\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)  # drop='first' to avoid multicollinearity\n",
    "X_categorical_encoded = encoder.fit_transform(X_categorical)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = encoder.get_feature_names_out(categorical_vars)\n",
    "X_categorical_encoded_df = pd.DataFrame(\n",
    "    X_categorical_encoded,\n",
    "    columns=feature_names,\n",
    "    index=df_processed.index\n",
    ")\n",
    "\n",
    "print(f\"   ✓ Applied One-Hot Encoding (drop='first')\")\n",
    "print(f\"   ✓ Encoded variables: {list(X_categorical_encoded_df.columns)}\")\n",
    "print(f\"   ✓ Number of encoded features: {len(feature_names)}\")\n",
    "\n",
    "# Display encoding info\n",
    "print(f\"\\n   Encoding details:\")\n",
    "for col in categorical_vars:\n",
    "    n_categories = X_categorical[col].nunique()\n",
    "    n_encoded = len([c for c in feature_names if col in c])\n",
    "    print(f\"     {col}: {n_categories} categories → {n_encoded} encoded features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48ff0842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Combining normalized and encoded features...\n",
      "   ✓ Combined feature matrix shape: (25782, 22)\n",
      "   ✓ Total features: 22\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5. COMBINE NORMALIZED CONTINUOUS AND ENCODED CATEGORICAL\n",
    "# ============================================================================\n",
    "print(\"\\n5. Combining normalized and encoded features...\")\n",
    "\n",
    "# Combine all features\n",
    "X_combined = pd.concat([X_continuous_scaled_df, X_categorical_encoded_df], axis=1)\n",
    "\n",
    "print(f\"   ✓ Combined feature matrix shape: {X_combined.shape}\")\n",
    "print(f\"   ✓ Total features: {X_combined.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8b0a6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying FAMD with up to 4 components...\n",
      "\n",
      "✓ FAMD transformation complete\n",
      "✓ Number of components: 4\n",
      "\n",
      "Explained variance (first 10 components):\n",
      "  Component 1: 25.53% (Cumulative: 25.53%)\n",
      "  Component 2: 24.99% (Cumulative: 50.52%)\n",
      "  Component 3: 24.90% (Cumulative: 75.42%)\n",
      "  Component 4: 24.58% (Cumulative: 100.00%)\n",
      "\n",
      "✓ 4 components explain 95% of variance\n",
      "  Total variance explained: 100.00%\n"
     ]
    }
   ],
   "source": [
    "if not PRINCE_AVAILABLE:\n",
    "    print(\"✗ ERROR: Cannot apply FAMD - prince package not available\")\n",
    "    print(\"  Please install: pip install prince\")\n",
    "else:\n",
    "    # For FAMD, we need the original data (not normalized/encoded)\n",
    "    # FAMD handles mixed data internally\n",
    "    X_famd_input = pd.concat([df_processed[continuous_vars], df_processed[categorical_vars]], axis=1)\n",
    "    \n",
    "    # Determine number of components (adjust as needed)\n",
    "    # Using components that explain most variance (max 20 or n_features-1)\n",
    "    max_components = min(20, X_famd_input.shape[1] - 1)\n",
    "    \n",
    "    print(f\"Applying FAMD with up to {max_components} components...\")\n",
    "    \n",
    "    # Create and fit FAMD\n",
    "    famd = prince.FAMD(\n",
    "        n_components=max_components,\n",
    "        n_iter=10,\n",
    "        copy=True,\n",
    "        check_input=True,\n",
    "        random_state=42,\n",
    "        engine='sklearn'\n",
    "    )\n",
    "    \n",
    "    # Fit and transform\n",
    "    X_famd_transformed = famd.fit_transform(X_famd_input)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    X_famd = pd.DataFrame(\n",
    "        X_famd_transformed,\n",
    "        columns=[f'FAMD_{i+1}' for i in range(X_famd_transformed.shape[1])],\n",
    "        index=df_processed.index\n",
    "    )\n",
    "    \n",
    "    n_components = X_famd.shape[1]\n",
    "    \n",
    "    # Calculate explained variance\n",
    "    # Calculate explained variance from eigenvalues\n",
    "    eigenvalues = famd.eigenvalues_\n",
    "    total_variance = eigenvalues.sum()\n",
    "    explained_variance = eigenvalues / total_variance\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "    # cumulative_variance = np.cumsum(explained_variance)\n",
    "    \n",
    "    print(f\"\\n✓ FAMD transformation complete\")\n",
    "    print(f\"✓ Number of components: {n_components}\")\n",
    "    print(f\"\\nExplained variance (first 10 components):\")\n",
    "    for i in range(min(10, n_components)):\n",
    "        print(f\"  Component {i+1}: {explained_variance[i]*100:.2f}% (Cumulative: {cumulative_variance[i]*100:.2f}%)\")\n",
    "    \n",
    "    # Find number of components for 95% variance\n",
    "    n_components_95 = np.where(cumulative_variance >= 0.95)[0]\n",
    "    if len(n_components_95) > 0:\n",
    "        n_95 = n_components_95[0] + 1\n",
    "        print(f\"\\n✓ {n_95} components explain 95% of variance\")\n",
    "        print(f\"  Total variance explained: {cumulative_variance[n_95-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31879a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. Creating final dataset...\n",
      "   ✓ Final dataset shape: (25782, 5)\n",
      "   ✓ Columns: ['FAMD_1', 'FAMD_2', 'FAMD_3', 'FAMD_4', 'Y']\n",
      "\n",
      "8. Saving processed data...\n",
      "   ✓ Saved to: ../../Data/processed\\data_continuous_famd.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 7. CREATE FINAL DATASET\n",
    "# ============================================================================\n",
    "print(\"\\n7. Creating final dataset...\")\n",
    "\n",
    "# Create final DataFrame with FAMD components and target\n",
    "df_final = X_famd.copy()\n",
    "df_final[target] = df_processed[target].values\n",
    "\n",
    "print(f\"   ✓ Final dataset shape: {df_final.shape}\")\n",
    "print(f\"   ✓ Columns: {list(df_final.columns)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 8. SAVE PROCESSED DATA\n",
    "# ============================================================================\n",
    "print(\"\\n8. Saving processed data...\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = '../../Data/processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = os.path.join(output_dir, 'data_continuous_famd.csv')\n",
    "df_final.to_csv(output_path, index=False, sep=';')\n",
    "\n",
    "print(f\"   ✓ Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f1d5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
